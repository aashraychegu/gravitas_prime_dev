{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import warnings\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cupy as cp\n",
    "import tqdm\n",
    "from scipy.interpolate import interp1d\n",
    "import torchaudio\n",
    "import torchaudio.functional as AF\n",
    "import torchaudio.transforms as AT\n",
    "\n",
    "# Jupyter notebook specific\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", \"Wswiglal-redir-stdio\")\n",
    "\n",
    "# Local imports - core functionality\n",
    "from coredldev.dataset import CoReDataset\n",
    "from coredldev.utilites.pipeline import pipeline\n",
    "\n",
    "# Local imports - finders and sources\n",
    "from coredldev.finders.distance_scaling.h5 import h5Finder\n",
    "from coredldev.sources.distance_scaling.h5 import h5Source\n",
    "\n",
    "# Local imports - preprocessing steps\n",
    "from coredldev.preprocessing.raw_postmerger.detector_angle_mixing import DetectorAngleMixing\n",
    "from coredldev.preprocessing.raw_postmerger.fast_detector_angle_mixing import detector_angle_mixing as fastdam\n",
    "from coredldev.preprocessing.raw_postmerger.distance_scale import distance_scale\n",
    "from coredldev.preprocessing.raw_postmerger.time_shift import time_shift\n",
    "from coredldev.preprocessing.to_tensor import to_tensor_clean\n",
    "from coredldev.preprocessing.ligo_noise.inject_noise import NoiseInjection1D as noise_injection_1d\n",
    "from coredldev.preprocessing.wavelet_transforms.morlet import MorletWaveletTransform\n",
    "from coredldev.preprocessing.whiten import TimeSeriesWhitener\n",
    "from pycbc.types import TimeSeries\n",
    "\n",
    "# Import PyCBC libraries\n",
    "import pycbc.noise\n",
    "import pycbc.psd\n",
    "import pycbc.filter\n",
    "\n",
    "import scipy\n",
    "from scipy.signal import welch\n",
    "\n",
    "from copy import deepcopy as dp\n",
    "\n",
    "# Load frequency values for plotting\n",
    "freqs = np.genfromtxt(\"freqs.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapoints, eosmap, remaining = h5Finder().get_datapoints()\n",
    "datapoints, eosmap, remaining = h5Finder(shiftpercents=[0],angles=[(0,0,0)],distances = [20]).get_datapoints()\n",
    "source = h5Source(eos_to_index_map=eosmap)\n",
    "dataset = CoReDataset(source, datapoints,lambda x: x)\n",
    "transformed_dataset = CoReDataset(source, datapoints,pipeline([DetectorAngleMixing(),distance_scale(),time_shift(),noise_injection_1d(),TimeSeriesWhitener(4,2),to_tensor_clean()]))\n",
    "morl = MorletWaveletTransform(freqs=np.loadtxt(\"freqs.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdam = DetectorAngleMixing()\n",
    "ds = distance_scale()\n",
    "ts = time_shift()\n",
    "ni = noise_injection_1d(psd_file = \"CE40-asd.txt\")\n",
    "ttc = to_tensor_clean()\n",
    "morl = MorletWaveletTransform(freqs=np.loadtxt(\"freqs.npy\"))\n",
    "# pycbcwhitener = whiten_signal(30)\n",
    "tswhitener = TimeSeriesWhitener(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[400]\n",
    "print(f\"0 {data[\"hplus\"].shape = }\")\n",
    "data1 = fdam(dp(data))\n",
    "print(f\"1 {data1[\"signal\"].shape = }\")\n",
    "data2 = ds(dp(data1))\n",
    "print(f\"2 {data2[\"signal\"].shape = }\")\n",
    "data3 = ts(dp(data2))\n",
    "print(f\"3 {data3[\"signal\"].shape = }\")\n",
    "data4 = ni(dp(data3))\n",
    "print(f\"4 {data4[\"signal\"].shape = }\")\n",
    "data5 = tswhitener(dp(data4))\n",
    "print(f\"5 {data5[\"signal\"].shape = }\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = data4[\"signal\"] \n",
    "sam_p = data4[\"params\"][\"sam_p\"]\n",
    "frequencies, asd = welch(signal, fs=1/sam_p, nperseg=20000)\n",
    "print(freqs.shape, asd.shape)\n",
    "fft = np.fft.rfft(signal)\n",
    "fft_freqs = np.fft.rfftfreq(len(signal), d=sam_p)\n",
    "print(fft.shape, fft_freqs.shape)\n",
    "asd_interpolated = np.interp(fft_freqs, frequencies, asd)\n",
    "print(asd_interpolated.shape)\n",
    "whitened_freqs = fft / np.sqrt(asd_interpolated) /(2*np.pi*(20_000)/sam_p+1)\n",
    "print(whitened_freqs.shape)\n",
    "whitened_ts = np.fft.irfft(whitened_freqs)\n",
    "print(whitened_ts.shape)\n",
    "plt.plot(whitened_ts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(signal)\n",
    "plt.show()\n",
    "analyze_signal_frequencies(signal, sam_p, \" \", True)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.loglog(frequencies, np.sqrt(asd))\n",
    "plt.title(\"Amplitude Spectral Density (ASD)\")\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"ASD (strain/âˆšHz)\")\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "plt.show()# Compute the ASD using Welch's method\n",
    "plt.loglog(frequencies, whitened_freqs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_signal_frequencies(signal, sampling_period,name,plot = False):\n",
    "    \"\"\"\n",
    "    Perform FFT analysis on a signal and extract frequency information\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signal : array-like\n",
    "        Time domain signal to analyze\n",
    "    sampling_period : float\n",
    "        The sampling period in seconds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (min_nonzero_freq, max_freq, freq_bins, fft_magnitude)\n",
    "        - min_nonzero_freq: minimum nonzero frequency with significant magnitude\n",
    "        - max_freq: maximum frequency with significant magnitude\n",
    "        - freq_bins: array of frequency bins\n",
    "        - fft_magnitude: array of magnitude values\n",
    "    \"\"\"\n",
    "    # Calculate sampling frequency\n",
    "    sampling_frequency = 1.0 / sampling_period\n",
    "    \n",
    "    # Calculate FFT\n",
    "    fft_result = np.fft.rfft(signal)\n",
    "    fft_magnitude = np.abs(fft_result)\n",
    "    \n",
    "    # Calculate frequency bins\n",
    "    n_samples = len(signal)\n",
    "    freq_bins = np.fft.rfftfreq(n_samples, d=sampling_period)\n",
    "    \n",
    "    # Find significant peaks (frequencies)\n",
    "    # Using a simple threshold approach - you might want to adjust this\n",
    "    threshold = 0.1 * np.max(fft_magnitude)\n",
    "    peak_indices = np.where(fft_magnitude > threshold)[0]\n",
    "    peak_freqs = freq_bins[peak_indices]\n",
    "    peak_magnitudes = fft_magnitude[peak_indices]\n",
    "    \n",
    "    # Sort peaks by magnitude\n",
    "    sort_indices = np.argsort(peak_magnitudes)[::-1]\n",
    "    sorted_freqs = peak_freqs[sort_indices]\n",
    "    sorted_magnitudes = peak_magnitudes[sort_indices]\n",
    "    \n",
    "    # Find the maximum frequency (using a small threshold to avoid noise)\n",
    "    noise_threshold = 1e-10 * np.max(fft_magnitude)\n",
    "    nonzero_indices = np.where(fft_magnitude > noise_threshold)[0]\n",
    "    \n",
    "    # Find the maximum frequency\n",
    "    if len(nonzero_indices) > 0:\n",
    "        max_freq_index = nonzero_indices[-1]\n",
    "        max_freq = freq_bins[max_freq_index]\n",
    "    else:\n",
    "        max_freq = 0.0\n",
    "    \n",
    "    # Find the minimum non-zero frequency (excluding DC component at index 0)\n",
    "    nonzero_indices = nonzero_indices[nonzero_indices > 0] if len(nonzero_indices) > 0 else []\n",
    "    if len(nonzero_indices) > 0:\n",
    "        min_nonzero_freq_index = nonzero_indices[0]\n",
    "        min_nonzero_freq = freq_bins[min_nonzero_freq_index]\n",
    "    else:\n",
    "        min_nonzero_freq = 0.0\n",
    "    \n",
    "    # Plot the results\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        # Plot 1: Full spectrum\n",
    "        plt.subplot(211)\n",
    "        plt.loglog(freq_bins, fft_magnitude)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.title(f'Frequency Spectrum: {name}')\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Magnitude')\n",
    "        \n",
    "        # Add vertical lines for min and max frequencies\n",
    "        plt.axvline(x=min_nonzero_freq, color='g', linestyle='--', \n",
    "                    label=f'Min non-zero freq: {min_nonzero_freq:.2f} Hz')\n",
    "        plt.axvline(x=max_freq, color='r', linestyle='--',\n",
    "                    label=f'Max freq: {max_freq:.2f} Hz')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot 2: Log scale view for better visibility of smaller components\n",
    "        plt.subplot(212)\n",
    "        plt.loglog(freq_bins, fft_magnitude)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.title('Frequency Spectrum (log scale)')\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Magnitude (log scale)')\n",
    "        \n",
    "        # Add vertical lines on log plot too\n",
    "        plt.axvline(x=min_nonzero_freq, color='g', linestyle='--', \n",
    "                    label=f'Min non-zero freq: {min_nonzero_freq:.2f} Hz')\n",
    "        plt.axvline(x=max_freq, color='r', linestyle='--',\n",
    "                    label=f'Max freq: {max_freq:.2f} Hz')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "    \n",
    "    # Print the min and max frequencies\n",
    "    print(f\"Signal name: {name}\")\n",
    "    print(f\"Minimum non-zero frequency: {min_nonzero_freq:.2f} Hz\")\n",
    "    print(f\"Maximum frequency: {max_freq:.2f} Hz\")\n",
    "    print()\n",
    "    \n",
    "    # Return min and max frequencies along with the frequency data\n",
    "    return min_nonzero_freq, max_freq, freq_bins, fft_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_whitening_frequency_analysis():\n",
    "    mins = []\n",
    "    maxs = []\n",
    "    bin_list = []\n",
    "    for n, (data,metadata) in enumerate(transformed_dataset):\n",
    "        minimum, maximum, bins, _ = analyze_signal_frequencies(data.numpy(),dataset[n][\"params\"][\"sam_p\"],f\"{n} | {metadata}\")\n",
    "        mins.append(minimum)\n",
    "        maxs.append(maximum)\n",
    "        bin_list.append(bins)\n",
    "        plt.show()\n",
    "    print(min(mins),max(maxs))\n",
    "    flattened_data = np.concatenate(bin_list)  # Replace `maxs` with your list of numpy arrays if different\n",
    "    plt.hist(mins,bins=100)\n",
    "    plt.title(\"Minimum Frequencies\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    plt.hist(maxs,bins=100)\n",
    "    plt.title(\"Maximum Frequencies\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    # Flatten the list of numpy arrays\n",
    "\n",
    "    # Plot a histogram of the flattened data\n",
    "    plt.hist(flattened_data, bins=1000, alpha=0.7, color='blue')\n",
    "    plt.hist(fspace, bins=1000, alpha=0.7, color='red')\n",
    "    plt.title(\"Histogram of Flattened Data\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "post_whitening_frequency_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect timing statistics for each step\n",
    "def benchmark():\n",
    "    fdam = DetectorAngleMixing()\n",
    "    ds = distance_scale()\n",
    "    ts = time_shift()\n",
    "    ni = noise_injection_1d()\n",
    "    ttc = to_tensor_clean()\n",
    "\n",
    "    # Initialize dictionaries to store timing data\n",
    "    timing_data = {\n",
    "        \"Initial data loading\": [],\n",
    "        \"Detector angle mixing\": [],\n",
    "        \"Distance scaling\": [],\n",
    "        \"Time shifting\": [],\n",
    "        \"Noise injection\": [],\n",
    "        \"To tensor clean\": [],\n",
    "        \"Total\": []\n",
    "    }\n",
    "\n",
    "    # Number of samples to process\n",
    "    num_samples = 10000\n",
    "\n",
    "    # Collect timing data\n",
    "    for i in range(num_samples):\n",
    "        # Initial data loading\n",
    "        start_time = time.time()\n",
    "        data = dataset[i]\n",
    "        data_loading_time = time.time() - start_time\n",
    "        timing_data[\"Initial data loading\"].append(data_loading_time)\n",
    "        \n",
    "        # Detector angle mixing\n",
    "        start_time = time.time()\n",
    "        data1 = fdam(data)\n",
    "        dam_time = time.time() - start_time\n",
    "        timing_data[\"Detector angle mixing\"].append(dam_time)\n",
    "        \n",
    "        # Distance scaling\n",
    "        start_time = time.time()\n",
    "        data2 = ds(data1)\n",
    "        ds_time = time.time() - start_time\n",
    "        timing_data[\"Distance scaling\"].append(ds_time)\n",
    "        \n",
    "        # Time shifting\n",
    "        start_time = time.time()\n",
    "        data3 = ts(data2)\n",
    "        ts_time = time.time() - start_time\n",
    "        timing_data[\"Time shifting\"].append(ts_time)\n",
    "        \n",
    "        # Noise injection\n",
    "        start_time = time.time()\n",
    "        data4 = ni(data3)\n",
    "        ni_time = time.time() - start_time\n",
    "        timing_data[\"Noise injection\"].append(ni_time)\n",
    "        \n",
    "        # To tensor clean\n",
    "        start_time = time.time()\n",
    "        data5 = ttc(data4)\n",
    "        ttc_time = time.time() - start_time\n",
    "        timing_data[\"To tensor clean\"].append(ttc_time)\n",
    "        \n",
    "        # Calculate total time\n",
    "        total_time = data_loading_time + dam_time + ds_time + ts_time + ni_time + ttc_time\n",
    "        timing_data[\"Total\"].append(total_time)\n",
    "\n",
    "    # Calculate statistics\n",
    "    stats = {}\n",
    "    for step, times in timing_data.items():\n",
    "        mean_time = np.mean(times)\n",
    "        std_time = np.std(times)\n",
    "        stats[step] = {\"mean\": mean_time, \"std\": std_time}\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nTiming Statistics (in seconds):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Step':<25} {'Mean':<15} {'Std Dev':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    for step, stat in stats.items():\n",
    "        print(f\"{step:<25} {stat['mean']:<15.6f} {stat['std']:<15.6f}\")\n",
    "\n",
    "    # Create a bar chart for visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    steps = list(stats.keys())\n",
    "    means = [stats[step]['mean'] for step in steps]\n",
    "    stds = [stats[step]['std'] for step in steps]\n",
    "\n",
    "    # Plot bar chart with error bars\n",
    "    bars = plt.bar(steps, means, yerr=stds, capsize=10)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Processing Step')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('Mean Processing Time per Step with Standard Deviation')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show percentage contribution to total time\n",
    "    total_mean = stats['Total']['mean']\n",
    "    print(\"\\nPercentage Contribution to Total Processing Time:\")\n",
    "    print(\"-\" * 50)\n",
    "    for step in steps[:-1]:  # Exclude the 'Total' step\n",
    "        percentage = (stats[step]['mean'] / total_mean) * 100\n",
    "        print(f\"{step:<25} {percentage:>6.2f}%\")\n",
    "\n",
    "    # Create a pie chart showing percentage contribution\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    labels = steps[:-1]  # Exclude the 'Total' step\n",
    "    sizes = [stats[step]['mean'] for step in labels]\n",
    "    percentages = [(size / total_mean) * 100 for size in sizes]\n",
    "\n",
    "    plt.pie(sizes, labels=[f\"{label}\\n({perc:.1f}%)\" for label, perc in zip(labels, percentages)], \n",
    "            autopct='%1.1f%%', startangle=140, shadow=True)\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    plt.title('Percentage Contribution of Each Step to Total Processing Time')\n",
    "    plt.show()\n",
    "#benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_steps():\n",
    "    # Create a proper symlog plot for signals from data1 to data5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot each signal with a different color and label\n",
    "    plt.plot(np.arange(len(data1[\"signal\"])), data1[\"signal\"], label=\"data1 - After Detector Angle Mixing\")\n",
    "    plt.plot(np.arange(len(data2[\"signal\"])), data2[\"signal\"], label=\"data2 - After Distance Scaling\")\n",
    "    plt.plot(np.arange(len(data3[\"signal\"])), data3[\"signal\"], label=\"data3 - After Time Shifting\")\n",
    "    plt.plot(np.arange(len(data4[\"signal\"])), data4[\"signal\"], label=\"data4 - After Noise Injection\")\n",
    "    plt.plot(np.arange(len(data5[\"signal\"])), data5[\"signal\"], label=\"data5 - After Whitening\")\n",
    "\n",
    "    # Set symlog scale for y-axis with appropriate parameters\n",
    "    plt.yscale('symlog', linthresh=1e-25)  # Set linear threshold appropriate for the data\n",
    "\n",
    "    # Add grid, legend, and labels\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Signal Comparison Across Processing Steps\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Signal Amplitude (symlog scale)\")\n",
    "\n",
    "    # Add a zoomed inset for data5 which has much larger values\n",
    "    from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "    axins = inset_axes(plt.gca(), width=\"40%\", height=\"30%\", loc=\"upper right\")\n",
    "    axins.plot(np.arange(len(data5[\"signal\"])), data5[\"signal\"], 'r-')\n",
    "    axins.set_title(\"Zoomed view of whitened signal\")\n",
    "    axins.grid(True, alpha=0.2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Create a second figure to compare the signals on separate subplots\n",
    "    fig, axes = plt.subplots(6, 1, figsize=(12, 15), sharex=True)\n",
    "\n",
    "    # Plot each signal on its own subplot\n",
    "    axes[0].plot(np.arange(len(data1[\"signal\"])), data1[\"signal\"], 'b-')\n",
    "    axes[0].set_title(\"After Detector Angle Mixing\")\n",
    "    # axes[0].set_yscale('symlog', linthresh=1e-25)\n",
    "    axes[0].grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "    axes[1].plot(np.arange(len(data2[\"signal\"])), data2[\"signal\"], 'g-')\n",
    "    axes[1].set_title(\"After Distance Scaling\")\n",
    "    # axes[1].set_yscale('symlog', linthresh=1e-25)\n",
    "    axes[1].grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "    axes[2].plot(np.arange(len(data3[\"signal\"])), data3[\"signal\"], 'r-')\n",
    "    axes[2].set_title(\"After Time Shifting\")\n",
    "    # axes[2].set_yscale('symlog', linthresh=1e-25)\n",
    "    axes[2].grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "    axes[3].plot(np.arange(len(data4[\"signal\"])), data4[\"signal\"], 'c-')\n",
    "    axes[3].set_title(\"After Noise Injection\")\n",
    "    # axes[3].set_yscale('symlog', linthresh=1e-25)\n",
    "    axes[3].grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "    axes[4].plot(np.arange(len(data5[\"signal\"])), data5[\"signal\"], 'm-')\n",
    "    axes[4].set_title(\"After Whitening\")\n",
    "    axes[4].grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "    axes[5].plot(np.arange(len(data5[\"signal\"])), data5[\"signal\"], 'm-')\n",
    "    axes[5].set_title(\"After Whitening (Symlog)\")\n",
    "    axes[4].set_yscale('symlog')\n",
    "    axes[5].grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "    # Add common labels\n",
    "    fig.add_subplot(111, frameon=False)\n",
    "    plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Signal Amplitude\")\n",
    "    plt.title(\"Signal Processing Steps Comparison\", fontsize=16, pad=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gravitas_prime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
